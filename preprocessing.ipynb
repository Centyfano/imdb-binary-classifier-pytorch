{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import ImdbDataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = ImdbDataset('data/aclImdb_v1.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_sentence</th>\n",
       "      <th>train_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There are many police dramas doing the rounds....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chilling, majestic piece of cinematic fright, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'd just like to say that i've seen this film ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This 1981 comedy still sparkles thanks to the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One doesn't get to enjoy this gem, the 1936 In...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>Pretty awful but watchable and entertaining. I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>Oh man is this movie bad. It flows horribly. T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>Joyce Reynolds seems a might grown-up for the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>I liked Boyle's performance, but that's about ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>i saw the film and i got screwed, because the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          train_sentence  train_label\n",
       "0      There are many police dramas doing the rounds....            1\n",
       "1      Chilling, majestic piece of cinematic fright, ...            1\n",
       "2      I'd just like to say that i've seen this film ...            1\n",
       "3      This 1981 comedy still sparkles thanks to the ...            1\n",
       "4      One doesn't get to enjoy this gem, the 1936 In...            1\n",
       "...                                                  ...          ...\n",
       "24995  Pretty awful but watchable and entertaining. I...            0\n",
       "24996  Oh man is this movie bad. It flows horribly. T...            0\n",
       "24997  Joyce Reynolds seems a might grown-up for the ...            0\n",
       "24998  I liked Boyle's performance, but that's about ...            0\n",
       "24999  i saw the film and i got screwed, because the ...            0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = {'train_sentence': imdb.train_sentences, 'train_label': imdb.train_labels}\n",
    "ts= {'test_sentence': imdb.test_sentences, 'test_label': imdb.test_labels}\n",
    "df_tr = pd.DataFrame(data=tr)\n",
    "df_ts = pd.DataFrame(data=ts)\n",
    "df_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imdb.test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'love', 'my', 'dog', '!', '!'], ['i', 'love', 'my', 'cat'], ['you', 'love', 'my', 'dog', '!'], ['do', 'you', 'think', 'my', 'dog', 'is', 'amazing', '?']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Vocab.__init__() got an unexpected keyword argument 'specials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m tokens \u001b[39m=\u001b[39m [tokenizer(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(tokens)\n\u001b[0;32m---> 11\u001b[0m vocab \u001b[39m=\u001b[39m torchtext\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mVocab(specials\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m<unk>\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m<pad>\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m<bos>\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m<eos>\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     12\u001b[0m vocab\u001b[39m.\u001b[39mbuild_vocab_from_iterator(tokens)\n\u001b[1;32m     13\u001b[0m word_index \u001b[39m=\u001b[39m vocab\u001b[39m.\u001b[39mstoi\n",
      "\u001b[0;31mTypeError\u001b[0m: Vocab.__init__() got an unexpected keyword argument 'specials'"
     ]
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english', )\n",
    "texts = [\n",
    "    'I love my dog!!',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "tokens = [tokenizer(text) for text in texts]\n",
    "print(tokens)\n",
    "\n",
    "vocab = torchtext.vocab.Vocab(specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.build_vocab_from_iterator(tokens)\n",
    "word_index = vocab.stoi\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m input_sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThis is an example sentence for testing purposes\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Apply the SentencePiece tokenizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mSentencePieceTokenizer(xlmr_spm_model_path)\n\u001b[1;32m      6\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer(input_sentence)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTokens: \u001b[39m\u001b[39m\"\u001b[39m, tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the input sentence\n",
    "input_sentence = \"This is an example sentence for testing purposes\"\n",
    "\n",
    "# Apply the SentencePiece tokenizer\n",
    "tokenizer = T.SentencePieceTokenizer(xlmr_spm_model_path)\n",
    "tokens = tokenizer(input_sentence)\n",
    "print(\"Tokens: \", tokens)\n",
    "# Output: Tokens: ['▁This', '▁is', '▁an', '▁example', '▁sentence', '▁for', '▁testing', '▁purposes']\n",
    "\n",
    "# Apply the VocabTransform\n",
    "vocab_transform = T.VocabTransform(load_state_dict_from_url(xlmr_vocab_path))\n",
    "indexed_tokens = vocab_transform(tokens)\n",
    "print(\"Indexed tokens: \", indexed_tokens)\n",
    "# Output: Indexed tokens: [238, 13, 62, 207, 2559, 29, 4061, 1331]\n",
    "\n",
    "# Truncate the sequence to a maximum length of 254 (to account for <bos> and <eos> tokens)\n",
    "truncate_transform = T.Truncate(254)\n",
    "truncated_tokens = truncate_transform(indexed_tokens)\n",
    "print(\"Truncated tokens: \", truncated_tokens)\n",
    "# Output: Truncated tokens: [238, 13, 62, 207, 2559, 29, 4061, 1331]\n",
    "\n",
    "# Add <bos> and <eos> tokens to the sequence\n",
    "add_token_transform1 = T.AddToken(bos_idx, begin=True)\n",
    "add_token_transform2 = T.AddToken(eos_idx, begin=False)\n",
    "bos_eos_tokens = add_token_transform2(add_token_transform1(truncated_tokens))\n",
    "print(\"Tokens with <bos> and <eos>: \", bos_eos_tokens)\n",
    "# Output: Tokens with <bos> and <eos>: [0, 238, 13, 62, 207, 2559, 29, 4061, 1331, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0\t word: my\t count: 5\n",
      "idx: 1\t word: love\t count: 3\n",
      "idx: 2\t word: I\t count: 2\n",
      "idx: 3\t word: is\t count: 2\n",
      "idx: 4\t word: dog!!\t count: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'my': 2, 'love': 3, 'I': 4, 'is': 5, 'dog!!': 6, '<OOV>': 1, '<PAD>': 0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = Counter()\n",
    "texts = [\n",
    "    'I love my dog!!',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?',\n",
    "    'my house is amazing'\n",
    "]\n",
    "for text in texts:\n",
    "    words = text.split()\n",
    "    word_counts.update(words)\n",
    "most_common = word_counts.most_common(5)\n",
    "for idx, (word, count) in enumerate(most_common):\n",
    "    print(f\"idx: {idx}\\t word: {word}\\t count: {count}\")\n",
    "word2index = {}\n",
    "word2index = {word: idx + 2 for idx, (word, count) in enumerate(most_common)}\n",
    "word2index['<OOV>'] = 1\n",
    "word2index['<PAD>'] = 0\n",
    "word2index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 2, word: my\n",
      "idx: 3, word: love\n",
      "idx: 4, word: I\n",
      "idx: 5, word: is\n",
      "idx: 6, word: dog!!\n",
      "idx: 1, word: <OOV>\n",
      "idx: 0, word: <PAD>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: 'my', 3: 'love', 4: 'I', 5: 'is', 6: 'dog!!', 1: '<OOV>', 0: '<PAD>'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word, idx, in word2index.items():\n",
    "    print(f\"idx: {idx}, word: {word}\")\n",
    "\n",
    "idx2word = {idx: word for word, idx, in word2index.items()}\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 3, 2, 6], [4, 3, 2, 1], [1, 3, 2, 1], [1, 1, 1, 2, 1, 5, 1], [2, 1, 5, 1]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "for text in texts: \n",
    "    words = text.split()\n",
    "    sequence = [word2index.get(word, 1) for word in words]\n",
    "    sequences.append(sequence)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 2, 'love': 3, 'I': 4, 'dog': 5, 'is': 6, 'cat': 7, '<oov>': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: 'my', 3: 'love', 4: 'I', 5: 'dog', 6: 'is', 7: 'cat', 1: '<oov>'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, num_words, oov_token=None, pad_token=None):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            word_counts.update(words)\n",
    "\n",
    "        most_common = word_counts.most_common(self.num_words)\n",
    "        first_tokens = [self.oov_token,self.pad_token]\n",
    "        first_tokens_nn = list(filter(lambda s:s is not None, first_tokens))\n",
    "        lens = len(first_tokens_nn)\n",
    "        self.word_index = {word: (idx+1) + lens for idx, (word, count) in enumerate(most_common)}\n",
    "\n",
    "        if lens > 0:\n",
    "            list_len = [i+1 for i in range(len(first_tokens_nn))]\n",
    "\n",
    "            if self.pad_token is not None:\n",
    "                self.word_index[self.pad_token] = list_len[0]\n",
    "            if self.oov_token is not None:\n",
    "                self.word_index[self.oov_token] = list_len[-1]\n",
    "            \n",
    "        self.index_word = {index: word for word, index in self.word_index.items()}\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            sequence = [self.word_index.get(word, 1) for word in words]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "\n",
    "\n",
    "texts = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?',\n",
    "    'my house is amazing'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=6, oov_token='<oov>')\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(word_index)\n",
    "index_word = tokenizer.index_word\n",
    "index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lists \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mone\u001b[39m\u001b[39m'\u001b[39m, ]\n\u001b[1;32m      2\u001b[0m lens \u001b[39m=\u001b[39m [i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(lists))]\n\u001b[0;32m----> 3\u001b[0m lens[\u001b[39m1\u001b[39;49m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lists = ['one', ]\n",
    "lens = [i+1 for i in range(len(lists))]\n",
    "lens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "sp = \"\"\"\n",
    "\"\"\"\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "\n",
    "print(ngrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM(nn.Module):\n",
    "    def __init__(self, vocab_size, ):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = imdb.create_dataset(split=0.2)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(self, val_split = None):\n",
    "        self.extract()\n",
    "        corpi = ['train', 'test']\n",
    "        train = []\n",
    "        test = []\n",
    "        val =  []\n",
    "        for corpus in corpi:\n",
    "            path = os.path.join(self.dataset_dir, corpus)\n",
    "            if corpus == 'train':\n",
    "                pos_sentences, pos_labels = self.append(path, 'pos', 1)\n",
    "                neg_sentences, neg_labels = self.append(path, 'neg', 0)\n",
    "                sentences = pos_sentences + neg_sentences\n",
    "                labels = pos_labels + neg_labels\n",
    "                train.append([sentences, labels])\n",
    "            elif corpus == 'test':\n",
    "                pos_sentences, pos_labels = self.append(path, 'pos', 1)\n",
    "                neg_sentences, neg_labels = self.append(path, 'neg', 0)\n",
    "                sentences = pos_sentences + neg_sentences\n",
    "                labels = pos_labels + neg_labels\n",
    "                test.append([sentences, labels])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0716e80c24abe7056858ceae577246b2382b10fd9dcfb4e83f0a9532ed042f33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
